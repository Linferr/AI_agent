# eval

Evaluation scaffolding (test cases + offline runner) lives here.

## Scripts

- `run_eval.py`: retrieval hit@k (BM25 vs vector)
- `run_eval_answer.py`: answer-level evaluation (citation consistency + coverage)
